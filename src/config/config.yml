# ModelArguments
model_name_or_path: klue/roberta-large
config_name: null
tokenizer_name: null
dpr: False
bm25: True

# DataTrainingArguments
dataset_name: "./data/train_dataset"
overwrite_cache: False
preprocessing_num_workers: null
max_seq_length: 384
pad_to_max_length: False
doc_stride: 128
max_answer_length: 30
eval_retrieval: True
num_clusters: 64
top_k_retrieval: 4
use_faiss: False
num_neg: 2
evaluate: True

# TrainingArguments
do_train: True
do_eval: False
do_predict: False
output_dir: "./output"
learning_rate: 2.0e-5
num_train_epochs: 3
per_device_train_batch_size: 16
per_device_eval_batch_size: 16
save_total_limit: 5
save_strategy: "epoch"
warmup_steps: 0
weight_decay: 0.01
logging_dir: "./logs"
logging_steps: 10
evaluation_strategy: "epoch"
fp16: True