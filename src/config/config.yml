# ModelArguments
model_name_or_path: "output" # "output" "klue/roberta-large"
config_name: null
tokenizer_name: null

# DataTrainingArguments
dataset_name: "data/test_dataset" # "data/test_dataset" "data/train_dataset"
overwrite_cache: False
preprocessing_num_workers: null
max_seq_length: 384
pad_to_max_length: False
doc_stride: 128
max_answer_length: 30
eval_retrieval: True
num_clusters: 64
top_k_retrieval: 10
use_faiss: False
use_bm25: True

# TrainingArguments
do_train: False
do_eval: False
do_predict: True
output_dir: "./output"
learning_rate: 5.0e-05
num_train_epochs: 3
per_device_train_batch_size: 16
per_device_eval_batch_size: 16
save_total_limit: 5
save_strategy: "epoch"
warmup_steps: 0
weight_decay: 0.01
logging_dir: "./logs"
logging_steps: 10
evaluation_strategy: "epoch"
fp16: True