# ModelArguments
model_name_or_path: "klue/bert-base"
config_name: null
tokenizer_name: null

# DataTrainingArguments
dataset_name: "data/train_dataset"
overwrite_cache: False
preprocessing_num_workers: null
max_seq_length: 384
pad_to_max_length: False
doc_stride: 128
max_answer_length: 30
eval_retrieval: True
num_clusters: 64
top_k_retrieval: 10
use_faiss: True

# TrainingArguments
do_train: True
do_eval: True
do_predict: False
output_dir: "./output"
learning_rate: 5.0e-05
num_train_epochs: 1
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
save_total_limit: 5
save_strategy: "epoch"
warmup_steps: 0
weight_decay: 0.01
logging_dir: "./logs"
logging_steps: 10
evaluation_strategy: "epoch"